```{r knitr, echo = FALSE}
knitr::opts_chunk$set(message = FALSE,
                      echo = FALSE,
                      warning = FALSE,
                      fig.align = 'left',
                      fig.height = 12,
                      fig.width = 15)
```


```{r libs, message = FALSE}
library(Rlabkey)
library(ImmuneSpaceR)
library(data.table)
library(ggplot2)
library(plotly)
library(readr)
library(DT)
library(tidytext)
library(wordcloud)
library(tm) # for stopwords
library(stringr)
library(rvest)
library(RColorBrewer)
```

```{r helpers}
subsetDF <- function(df, exclCol, exclList, dateCol, from, to, breaks){
  df <- df[ (df[[dateCol]] > from & df[[dateCol]] < to) & !df[[exclCol]] %in% exclList ]
  tmp <- as.POSIXct(df[[dateCol]], format = "%Y-%m-%d", tz = "UTC")
  df[[dateCol]] <- as.Date(cut(tmp, breaks = breaks), format = "%Y-%m-%d")
  return(df)
}

getCurrentRDS <- function(filename){
  allFiles <- list.files(subdir)
  targetFiles <- allFiles[ grepl(filename, allFiles) ]
  mostRecentFile <- sort(targetFiles, decreasing = TRUE)[[1]]
  tmp <- readRDS(file.path(subdir, mostRecentFile))
}
```

```{r global-vars}
# Location of cronjob artifacts
subdir <- "/share/files/Studies/R_API_resources/"

# testing only
# subdir <- "~/Documents/FHCRC/MonitorISData/"
# labkey.url.params <- list(
#   from = "2018-01-01",
#   to = "2020-01-01",
#   by = "week",
#   plotType = "bar"
# )
# labkey.url.base <- "https://test.immunespace.org"
```

```{r preprocess}
# Set dates for subsetting. 
# `From` - Jan 1, 2016 because this is when ImmuneSpace went live
# `To` - today
format <- "%Y-%m-%d"

from <- ifelse(labkey.url.params$from == "", "2016-01-01", labkey.url.params$from)
from <- as.POSIXct(from, format = format)

to <- ifelse(labkey.url.params$to == "", Sys.Date(), labkey.url.params$to)
to <- as.POSIXct(to, format = format)

if (from > to){
  stop("Please select a start date that is earlier than the end date.")
}

plotType <- ifelse(labkey.url.params$plotType %in% c("bar", "line"),
                   labkey.url.params$plotType,
                   "bar")

dateBy <- ifelse(labkey.url.params$by %in% c("day", "week", "month", "year"),
                 labkey.url.params$by,
                 "day")

# create standard `breaks` definition so that regardless of data there will be
# common axis labels
breaks <- seq(from, to, by = dateBy)

# colFilter from `export as Script` in schema-browser
# Use PROD always as many of excluded users are 'inactive' on TEST
usersToExclude <- labkey.selectRows(baseUrl = "https://www.immunespace.org",
                            folderPath = "/home",
                            schemaName = "core",
                            queryName = "SiteUsers",
                            viewName = "",
                            colFilter = makeFilter(c("Groups/Group$SName",
                                                     "CONTAINS_ONE_OF",
                                                     "Developers;Administrators;LabKey;excludeFromLogs")),
                            containerFilter = NULL)


# Admin emails no longer in the DB
oldAdminEmails <- c("rsautera@fhcrc.org",
                    "ldashevs@scharp.org")

# Vectors of people to exclude from counts
exclusionNames <- paste0(usersToExclude$`Display Name`, collapse = ";")
exclusionEmails <- c(usersToExclude$Email, oldAdminEmails)
exclusionIds <- usersToExclude$`User Id`
```

# User Interface

## 1. New Users: How many new users are we attracting over time?
```{r newusers}
users <- data.table(labkey.selectRows(baseUrl = labkey.url.base,
                                      folderPath = "/",
                                      schemaName = "core",
                                      queryName = "SiteUsers",
                                      viewName = ""))
users <- subsetDF(df = users, 
                  exclCol = "User Id", 
                  exclList = exclusionIds, 
                  dateCol = "Created", 
                  from = from, to = to, breaks = breaks)

# New users over time
usersByTime <- users[, list(N = .N), by = Created ]
p <- usersByTime %>%
  plot_ly(x = ~Created,
          y = ~N) %>%
  layout(xaxis = list(title = paste0("By ", dateBy)),
         yaxis = list(title = "Users"),
         title = "New Users over Time")
if (plotType == "line") {
  p %>% add_lines()
} else {
  p %>% add_bars()
}
```
We gained **`r round(mean(usersByTime$N, na.rm = TRUE), 2)` new users** per *`r dateBy`* on average from `r from` to `r to`.

<br>

## 2. Total Users over time: How many total users do we have?
```{r total-users}
setorder(usersByTime, Created)
totalUsers <- usersByTime[ , cumsum := cumsum(N)]

totalUsers %>%
  plot_ly(x = ~Created, y = ~cumsum) %>%
  layout(xaxis = list(title = paste0("By ", dateBy)),
         yaxis = list(title = "Cumulative users"),
         title = "Userbase Over Time") %>%
  add_lines(line = list(shape = "spline"))
```
<br>

## 3. User Segments over time: How are total users split up according to their email suffix?
```{r total-users-by-segment}
# total user background breakdown by segment
# Some oddball addresses are left with NA, but this captures > 95% of users
edu <- c("\\.(edu|gov|org)$")
personal <- c("@(gmail|yahoo|hotmail|aol|verizon)")
biz <- c("@([A-z]+|[A-z]+-[A-z]+)\\.com$")
intl <- c("\\.[a-z]{2}$")
users$category <- sapply(users$Email, function(x){
  if (grepl(edu, x)){
    return("EDU")
  }else if (grepl(personal, x)) {
    return("PERSONAL")
  }else if (grepl(biz, x)) {
    return("BUSINESS")
  }else if (grepl(intl, x)) {
    return("INTERNATIONAL")
  }else{
    return(NA)
  }
})
userSegmentByTime <- users[, list(N = .N), by = .(Created, category) ]
setorder(userSegmentByTime, Created)
totalSegmentByTime <- userSegmentByTime[ , cumsum := cumsum(N), by = category ]
spreadSegmentByTime <- dcast(totalSegmentByTime, 
                             Created ~ category, 
                             value.var = "cumsum")

p <- plot_ly(spreadSegmentByTime, x = ~Created) %>%
  add_lines(y = ~BUSINESS, name = 'Corporate', line = list(shape = "spline"), connectgaps = TRUE) %>%
  add_lines(y = ~EDU, name = 'Educational', line = list(shape = "spline"), connectgaps = TRUE) %>%
  add_lines(y = ~PERSONAL, name = 'Personal', line = list(shape = "spline"), connectgaps = TRUE) %>%
  add_lines(y = ~INTERNATIONAL, name = 'International', line = list(shape = "spline"), connectgaps = TRUE) %>%
  layout(xaxis = list(title = paste0("By ", dateBy)),
         yaxis = list(title = "Cumulative Users"),
         title = "Growth of User Segments over Time") 
p
```
<br>

## 4. Active users over time:  How many people are logging into the UI at any given time?
```{r activeusers}
allLogins <- data.table(labkey.selectRows(baseUrl = labkey.url.base,
                            folderPath = "/",
                            schemaName = "auditLog",
                            queryName = "UserAuditEvent",
                            viewName = "",
                            colFilter = makeFilter(c("Comment",
                                                     "CONTAINS",
                                                     "Database authentication")),
                            containerFilter= "AllFolders"))

cpAllLogins <- copy(allLogins)
currentLogins <- subsetDF(df = cpAllLogins, 
                  exclCol = "Created By", 
                  exclList = exclusionIds, 
                  dateCol = "Date", 
                  from = from, to = to, breaks = breaks)

activeUsersByTime <- copy(currentLogins)
activeUsersByTime <- activeUsersByTime[ , list(n = length(unique(`Created By`))), 
                                          by = Date]

p <- activeUsersByTime %>%
  plot_ly(x = ~Date,
          y = ~n) %>%
  layout(xaxis = list(title = paste0("By ", dateBy)),
         yaxis = list(title = "Users"),
         title = "Active Users over Time")
if (plotType == "line") {
  p %>% add_lines()
} else {
  p %>% add_bars()
}
```
We had **`r round(mean(activeUsersByTime$n, na.rm = TRUE), 2)` active users** per *`r dateBy`* on average from `r from` to `r to`.

<br>

## 6. Adjusted individual logins: What is the typical user's experience in terms of logins after the creation date? 
```{r typical-user-experience}
# Subset logins by those in the created table
allLogins <- allLogins[ !allLogins$`Created By` %in% exclusionIds ]
tmp <- allLogins[ allLogins$User %in% unique(users$`User Id`) ]

# Get all logins and make login_day as date - day0
tmp2 <- tmp[ , Date := cut(as.POSIXct(tmp$Date, format="%Y-%m-%d", tz="UTC"), breaks = "day") ]
tmp2$postBaseline <- apply(tmp2, 1, function(x){ 
  as.double(as.Date(x["Date"]) - as.Date(users$Created[ users$`User Id` == x["User"] ]))
})

# Subset to postBaseline
tmp2 <- tmp2[ tmp2$postBaseline > 0 ]

# Add weeks
tmp2$week <- sapply(tmp2$postBaseline, function(x){ floor(x / 7 + 1) })

# Get counts of users logging in on each post-baseline by week
tmp3 <- data.frame(matrix(data = 0, 
                          nrow = length(unique(tmp2$User)), 
                          ncol = max(unique(tmp2$week))))
rownames(tmp3) <- sort(unique(tmp2$User))
colnames(tmp3) <- seq(1:max(unique(tmp2$week)))

# By week
for( i in 1:nrow(tmp2)) {
  rw <- match(tmp2$User[[i]], rownames(tmp3))
  cl <- match(tmp2$week[[i]], colnames(tmp3))
  tmp3[ rw, cl ] <- 1
}

# Users that do log back in during their first week
usersCreated <- length(unique(users$`User Id`))

returns <- tmp3[ tmp3$`1` != 0, ]
retWk1 <- dim(returns)[[1]]
sixMonthsRet <- apply(returns, 1, function(x){ sum(x[2:27]) > 0 })
smr <- sum(sixMonthsRet)

noReturns <- tmp3[ tmp3$`1` == 0, ]
noRetWk1 <- dim(noReturns)[[1]]
sixMonthsNoRet <- apply(noReturns, 1, function(x){ sum(x[2:27]) > 0 })
smnr <- sum(sixMonthsNoRet)

tbl <- data.frame(a = c(usersCreated, retWk1, smr), 
                  b = c(usersCreated, retWk1, smr))
colnames(tbl) <- c("Count", "Percentage of Total Users")
rownames(tbl) <- c("Total Users", "Users returning within first week", "First Week Returns coming back within first 6 months")
tbl[,2] <- 100* round(tbl[,2]/usersCreated, 2)
datatable(tbl)
```

<br>

## 7. Table of top users
```{r usertable}
loginsByUser <- currentLogins[ , list(count = .N), by = .(`Created By`)]
loginsByUser$email <- users$Email[ match(loginsByUser$`Created By`, users$`User Id`)]
loginsByUser <- loginsByUser[ !is.na(email)]
setorder(loginsByUser, -count)
datatable(loginsByUser)
```

<br>

***

# ImmuneSpaceR

## 8. Downloads (new ISR Users)
```{r isr-bioc}
BIOC <- data.table(read.table("https://www.bioconductor.org/packages/stats/bioc/ImmuneSpaceR/ImmuneSpaceR_stats.tab", header = TRUE))
BIOC[, Date := as.Date(paste(Year, Month, "01", sep = "-"), format = "%Y-%b-%d")]
BIOC <- BIOC[ !is.na(Date) & Date >= as.Date(from) & Date <= as.Date(to)  ]
setorder(BIOC, Date)

p <- BIOC %>%
  plot_ly(x = ~Date,
          y = ~Nb_of_distinct_IPs) %>%
  layout(xaxis = list(title = paste0("By ", dateBy)),
         yaxis = list(title = "Downloads"),
         title = "Downloads in Bioconductor over Time")
if (plotType == "line") {
  p %>% add_lines()
} else {
  p %>% add_bars()
}
```

<br>

```{r tomcat-logs}
# To determine how ImmuneSpaceR has been used, we parse the server logs. Since the server logs
# all types of GET / POSTS and other requests, there is a fair amount of filtering that must 
# be done to find the requests that help us understand usage.
#
# These logs are created by Tomcat (server software) and written out to `/labkey/apps/tomcat/logs/`
# on the webserve machine. Since they cannot be accessed by the Rserve, we need to copy them to
# `/share` by setting up a cron job on `wsP/T` as `immunespace`.
#
# crontab -e
# Add this line
# 00 0-23/6 * * * rsync -a -v /labkey/apps/tomcat/logs/localhost_access_log.* /share/tomcat-logs/
# This will sync logs to `/share/tomcat-logs/` every six hour.
# 
# The logs are parsed using the `ImmuneSpaceCronjobs` package
```


```{r isr}
ISR_inits <- getCurrentRDS("ISR_inits")

# create var for conditional eval of following chunks and r statements
ISR_inits_present <- nrow(ISR_inits) > 0
usersOverTimeMsg <- 0
```


## 9. ImmuneSpaceR Users Over Time
```{r isr-user, eval=ISR_inits_present}
# Filter to single connection per user per day for accurate aggregation
ISR_inits[, date2 := as.Date(as.POSIXct(date2, format="%Y-%m-%d", tz="UTC")) ]
ISR_inits <- ISR_inits[ !X12 %in% exclusionEmails ]
ISR_inits <- ISR_inits[ !is.na(date2) & date2 >= as.Date(from) & date2 <= as.Date(to)  ]
ISR_users <- ISR_inits[ , list(X12, date2) ]
ISR_users <- ISR_users[ !duplicated(ISR_users) ]


# Group by Date break
ISR_usersOverTime <- ISR_users[ , Date := as.Date(cut(date2, breaks = labkey.url.params$by)) ]
ISR_usersOverTime <- ISR_usersOverTime[ , list(Users = .N), by = Date ]

p <- ISR_usersOverTime %>%
  plot_ly(x = ~Date,
          y = ~Users) %>%
  layout(xaxis = list(title = paste0("By ", dateBy)),
         yaxis = list(title = "Users"),
         title = "ISR users over time")
if (plotType == "line") {
  p %>% add_lines()
} else {
  p %>% add_bars()
}

usersOverTimeMsg <- ISR_usersOverTime$Users
```
We had **`r round(mean(usersOverTimeMsg, na.rm = TRUE), 2)` ISR users** per *`r dateBy`* on average from `r from` to `r to`.

<br>

## 11. Top ISR users
```{r isr-top-users, eval=ISR_inits_present}
topIsrUsers <- ISR_inits[, list(count = .N), by = X12]
setnames(topIsrUsers, colnames(topIsrUsers), c("Email", "Logins"))
setorder(topIsrUsers, -Logins)
datatable(topIsrUsers)
```
<br>

***

# Content - what are users looking for?

## 12. Search Queries
```{r searches-wordcloud}
searches <- data.table(labkey.selectRows(baseUrl = labkey.url.base,
                              folderPath = "/home",
                              schemaName = "auditLog",
                              queryName = "SearchAuditEvent",
                              viewName = "",
                              colSort = "Created",
                              colFilter = makeFilter(c("Query",
                                                       "DOES_NOT_CONTAIN",
                                                       "\\"),
                                                     c("Query",
                                                       "DOES_NOT_CONTAIN",
                                                       "1234"),
                                                     c("CreatedBy/DisplayName",
                                                       "NOT_IN",
                                                       exclusionNames)),
                              containerFilter = NULL))
searches <- searches[ `Created By` != 0 & Date >= from & Date <= to, ]
searches[ , id := paste(Date, `Created By`, sep = "-") ]
searches <- searches[, list(id, Query)]

freq <- tidytext::unnest_tokens(searches, output = word, input = Query)
freq <- freq[ , list(count = .N), by = word]
stopWords <- tm::stopwords()
freq <- freq[ !word %in% stopWords ]

pal <- brewer.pal(9,"YlGnBu")
pal <- pal[-(1:4)]

if (nrow(freq) > 0) {
  wordcloud::wordcloud(words = freq$word,
                       freq = freq$count,
                       max.words = 30,
                       rot.per = 0,
                       random.order = FALSE,
                       colors = pal)
} else {
  wordcloud::wordcloud(words = "no search",
                       freq = 1)
}
```
<br>

## 13. Most common search queries
```{r wordtable}
setorder(freq, -count)
datatable(freq, width = 600)
```
<br>

## 14. Studies Available
```{r studycount}
# Number of available studies is defined by the count of study folders created
folders <- data.table(labkey.selectRows(baseUrl = labkey.url.base,
                             folderPath = "/",
                             schemaName = "auditLog",
                             queryName ="ContainerAuditEvent",
                             colNameOpt = "rname",
                             containerFilter = "AllFolders"))
sdys <- folders[ !is.na(container) & grepl("Folder SDY[0-9].*created$", comment),
                 list(created, comment) ]
sdys[ , study := gsub("\\s.*$", "", gsub("^.*SDY", "SDY", comment)) ]
setorder(sdys, created)
sdys[ , count := .I ]

sdys %>%
  plot_ly(x = ~created, y = ~count) %>%
  layout(xaxis = list(title = "Date"),
         yaxis = list(title = "Studies"),
         title = "Number of studies available") %>%
  add_lines(line = list(shape = "hv"))
```
<br>

## 15. Studies accessed via ISR
```{r isr-study, eval=ISR_inits_present}
ISR_study <- ISR_inits[ , list(count = .N), by = study]
setorder(ISR_study, -count)
datatable(ISR_study,
          colnames = c("Study", "ISR connections"),
          caption = paste0("From ", from, " to ", to),
          width = 600)
```
<br>

## 16. Studies accessed via UI (pageviews)
```{r study-views}
log_study <- getCurrentRDS("log_study")
log_study[, date2 := as.Date(as.POSIXct(date2, format="%Y-%m-%d", tz="UTC")) ]
log_study <- log_study[ !is.na(date2) & date2 >= as.Date(from) & date2 <= as.Date(to)  ]
log_study <- log_study[ , Date := as.Date(cut(date2, breaks = labkey.url.params$by)) ]
log_study_summarized <- log_study[ , list(Views = .N), by = study ]

datatable(log_study_summarized,
          colnames = c("Study", "Unique pageviews"),
          caption = paste0("From ", from, " to ", to),
          width = 600)
```
<br>

## 17. Modules accessed via UI (pageviews)
```{r module-views}
log_module <- getCurrentRDS("log_module")
log_module[, date2 := as.Date(as.POSIXct(date2, format="%Y-%m-%d", tz="UTC")) ]
log_module <- log_module[ !is.na(date2) & date2 >= as.Date(from) & date2 <= as.Date(to)  ]
log_module <- log_module[ , Date := as.Date(cut(date2, breaks = labkey.url.params$by)) ]
log_module_summarized <- log_module[ , list(Views = .N), by = module ]

datatable(log_module_summarized,
          colnames = c("Module", "Unique pageviews"),
          caption = paste0("From ", from, " to ", to),
          width = 600)
```
<br>

## 18. Reports accessed via UI (pageviews)
```{r reports-views}
# Get study-specific reports, incl. IS1 using search string based on minimal URL
# then update the report column match study id.
log_reports <- getCurrentRDS("log_reports")
log_reports[, date2 := as.Date(as.POSIXct(date2, format="%Y-%m-%d", tz="UTC")) ]
log_reports <- log_reports[ !is.na(date2) & date2 >= as.Date(from) & date2 <= as.Date(to)  ]
log_reports <- log_reports[ , Date := as.Date(cut(date2, breaks = labkey.url.params$by)) ]
log_reports_summarized <- log_reports[ , list(Views = .N), by = report ]

datatable(log_reports_summarized,
          colnames = c("Report", "Unique pageviews"),
          caption = paste0("From ", from, " to ", to),
          width = 600)
```
<br>

## 19. Rstudio accessed via UI (pageviews)
```{r rstudio}
log_rstudio <- getCurrentRDS("log_rstudio")
log_rstudio <- log_rstudio[ !is.na(Date) & Date >= as.Date(from) & Date <= as.Date(to) ]

p <- log_rstudio %>%
  plot_ly(x = ~Date,
          y = ~Sessions) %>%
  layout(xaxis = list(title = paste0("By ", dateBy)),
         yaxis = list(title = "Sessions"),
         title = "RStudio sessions over time")
if (plotType == "line") {
  p %>% add_lines()
} else {
  p %>% add_bars()
}
```
We had **`r round(mean(log_rstudio$Sessions, na.rm = TRUE), 2)` RStudio sessions** per *`r dateBy`* on average from `r from` to `r to`.

***

## 20. Google Analytics Plots
```{r googleAnalytics}
gaData <- getCurrentRDS("googleAnalyticsArtifact")
gaData <- gaData[ !is.na(date) & date >= as.Date(from) & date <= as.Date(to) ]

# Manually curated 'best guesses'
gaData$condensedSource <- sapply(gaData$fullReferrer, function(x){
  if(grepl("bioconductor",x)){
    return("Bioconductor")
  }else if(grepl("cran", x)){
    return("Cran")
  }else if(grepl("immport", x)){
    return("ImmPort")
  }else if(grepl("sciencemag|nature\\.com", x)){
    return("Journals")
  }else if(grepl("immuneprofiling", x)){
    return("HIPC")
  }else if(grepl("immunedata", x)){
    return("ImmuneData")
  }else if(grepl("mail|exchange\\.fhcrc", x)){
    return("Email Link")
  }else if(grepl("rglab|fhcrc|rdrr\\.io|RGLab/(ImmuneSignatures|ImmuneSpaceR)", x)){
    return("RGLab Domain")
  }else if(grepl("google|baidu|bing|duckduckgo|yandex", x)){
    return("Search Engine")
  }else if(grepl("t\\.co", x)){
    return("Twitter")
  }else if(grepl("facebook", x)){
    return("Facebook")
  }else if(grepl("ncbi\\.nlm", x)){
    return("NCBI")
  }else if(grepl("niaid\\.nih", x)){
    return("NIAID")
  }else{
    return("Other")
  }
})

# summarize usageSessions by condensedSource by day
gaData[, shortDate := format(as.Date(date), "%Y-%m")]
sourceSummary <- gaData[, list(users = sum(usageSessions)), by = c("condensedSource", "shortDate") ]

# melt data so each day has values for every condensedSource
sourceSummaryWide <- dcast(sourceSummary, shortDate ~ condensedSource, value.var = "users")
sourceSummaryWide[ is.na(sourceSummaryWide)] <- 0
setorder(sourceSummaryWide, shortDate)

# Handle missing data
expectedCols <- c("Bioconductor",
                  "Cran",
                  "Email Link",
                  "Facebook",
                  "HIPC",
                  "ImmPort",
                  "ImmuneData",
                  "Journals",
                  "NCBI",
                  "NIAID",
                  "RGLab Domain",
                  "Search Engine",
                  "Twitter",
                  "Other")
missingCols <- setdiff(expectedCols, colnames(sourceSummaryWide))
for(column in missingCols){
  sourceSummaryWide[[column]] <- 0
}

# Stacked area chart shows unique user sessions from each condensed source per month
someColors <- brewer.pal(12, "Paired")
addlColors <- brewer.pal(3, "Greys")[1:2]
colors <- rev(c(someColors, addlColors))

p <- plot_ly(sourceSummaryWide, x = ~shortDate, y = ~Bioconductor,
               name = 'Bioconductor',
               type = 'scatter',
               mode = 'none',
               stackgroup = 'one',
               fillcolor = colors[[1]])
p <- p %>% add_trace(y = ~Cran, name = 'CRAN', fillcolor = colors[[2]])
p <- p %>% add_trace(y = ~`Email Link`, name = 'Email Link', fillcolor = colors[[3]])
p <- p %>% add_trace(y = ~Facebook, name = 'Facebook', fillcolor = colors[[4]])
p <- p %>% add_trace(y = ~HIPC, name = 'HIPC', fillcolor = colors[[5]])
p <- p %>% add_trace(y = ~ImmPort, name = 'ImmPort', fillcolor = colors[[6]])
p <- p %>% add_trace(y = ~ImmuneData, name = 'ImmuneData', fillcolor = colors[[7]])
p <- p %>% add_trace(y = ~Journals, name = 'Journals', fillcolor = colors[[8]])
p <- p %>% add_trace(y = ~NCBI, name = 'NCBI Database', fillcolor = colors[[9]])
p <- p %>% add_trace(y = ~NIAID, name = 'NIAID', fillcolor = colors[[10]])
p <- p %>% add_trace(y = ~`RGLab Domain`, name = 'RGLab Domain', fillcolor = colors[[11]])
p <- p %>% add_trace(y = ~`Search Engine`, name = 'Search Engine', fillcolor = colors[[12]])
p <- p %>% add_trace(y = ~Twitter, name = 'Twitter', fillcolor = colors[[13]])
p <- p %>% add_trace(y = ~Other, name = 'Other', fillcolor = colors[[14]])
p <- p %>% layout(title = 'Number of users referred to ImmuneSpace from different sites or links',
                      xaxis = list(title = "",
                                   showgrid = FALSE),
                      yaxis = list(title = "Unique users per Month",
                                   showgrid = FALSE))

p

# Monthly bounce rate plot
bouncesSummary <- gaData[, list(bounces = sum(bounces),
                                totalSessions = sum(usageSessions)),
                            by = "shortDate"]
bouncesSummary[, bounceRate := round((bounces / totalSessions) * 100, 2) ]
b <- plot_ly(bouncesSummary, x = ~shortDate, y = ~bounceRate,
               name = 'Bounce Rate',
               type = 'scatter',
               mode = 'none',
               stackgroup = '1',
               fillcolor = colors[[5]])
b <- b %>% layout(title = 'Percent of unique user sessions not moving beyond first page each month',
                      xaxis = list(title = "",
                                   showgrid = FALSE),
                      yaxis = list(title = "Bounce Rate",
                                   showgrid = FALSE,
                                   range = c(0, 100)))

b
```

# Debugging Messages

## Missing access logs
```{r debugging-messages}
# from_to$date[unlist(lapply(logs_list, is.null))]
```
